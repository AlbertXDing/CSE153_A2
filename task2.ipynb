{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "729488ce-753f-4a56-8921-dc78210606cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import miditoolkit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "import random\n",
    "import pretty_midi\n",
    "\n",
    "from symusic import Score\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from midiutil import MIDIFile\n",
    "from glob import glob\n",
    "# used chatgpt to help me generate some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdde5fbd-0de9-4c79-b127-f5abe51aa3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Processing the midi files\n",
    "midi_files = glob('nes_midis/*')\n",
    "print(len(midi_files))\n",
    "\n",
    "config = TokenizerConfig(num_velocities=1)\n",
    "tokenizer = REMI(config)\n",
    "tokenizer.train(vocab_size = 2000, files_paths=midi_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be7dc10-7a6f-47ca-a2af-08fc4f21270b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert\\AppData\\Roaming\\Python\\Python312\\site-packages\\pretty_midi\\pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "instruments = {}\n",
    "bad_files = []\n",
    "\n",
    "for file in midi_files:\n",
    "    try:\n",
    "        midi = pretty_midi.PrettyMIDI(file)\n",
    "        for instrument in midi.instruments:\n",
    "            name = pretty_midi.program_to_instrument_name(instrument.program)\n",
    "            instruments[name] = instruments.get(name, 0) + 1\n",
    "    except Exception as e:\n",
    "        bad_files.append(file)\n",
    "\n",
    "instrument_popularity = sorted(instruments, key = lambda k: instruments[k], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d0b756-bd60-4910-80f3-0ca8cf1f1074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9a18fa4-a2ef-4c0b-b33a-81a7cd5851d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Model setup complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import miditoolkit\n",
    "import numpy as np\n",
    "from miditok import REMI, TokSequence\n",
    "\n",
    "# ===== Dataset =====\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, files, tokenizer, max_len=1024):\n",
    "        self.samples = []\n",
    "        self.tokenizer = tokenizer  # Store tokenizer reference\n",
    "        \n",
    "        for path in files:\n",
    "            try:\n",
    "                midi = miditoolkit.MidiFile(path)\n",
    "                if len(midi.instruments) < 1:\n",
    "                    continue\n",
    "                \n",
    "                # Use miditok's tokenization method correctly\n",
    "                mel_tokens = tokenizer(midi)  # This returns token IDs or TokSequence\n",
    "                \n",
    "                # Debug: print the type and structure\n",
    "                if len(self.samples) == 0:  # Only print for first file\n",
    "                    print(f\"Tokenizer output type: {type(mel_tokens)}\")\n",
    "                    if hasattr(mel_tokens, '__len__'):\n",
    "                        print(f\"Length: {len(mel_tokens)}\")\n",
    "                    if isinstance(mel_tokens, (list, tuple)) and len(mel_tokens) > 0:\n",
    "                        print(f\"First element type: {type(mel_tokens[0])}\")\n",
    "                        if hasattr(mel_tokens[0], '__dict__'):\n",
    "                            print(f\"First element attributes: {list(mel_tokens[0].__dict__.keys())}\")\n",
    "                \n",
    "                # Handle different tokenization output formats\n",
    "                if isinstance(mel_tokens, list):\n",
    "                    if len(mel_tokens) == 0:\n",
    "                        continue\n",
    "                    # Could be list of TokSequence objects or list of integers\n",
    "                    if hasattr(mel_tokens[0], 'ids'):\n",
    "                        mel_ids = mel_tokens[0].ids  # TokSequence object\n",
    "                    else:\n",
    "                        mel_ids = mel_tokens  # List of integers\n",
    "                elif hasattr(mel_tokens, 'ids'):\n",
    "                    mel_ids = mel_tokens.ids  # Single TokSequence object\n",
    "                elif isinstance(mel_tokens, (list, tuple)):\n",
    "                    mel_ids = list(mel_tokens)  # Direct token list\n",
    "                else:\n",
    "                    print(f\"Unexpected tokenizer output type: {type(mel_tokens)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Truncate to reasonable length\n",
    "                mel_ids = mel_ids[:max_len-1]  # Leave room for special tokens\n",
    "                \n",
    "                # Add BOS token if it exists\n",
    "                bos_token = None\n",
    "                eos_token = None\n",
    "                pad_token = 0\n",
    "                \n",
    "                # Try to get special tokens safely\n",
    "                try:\n",
    "                    if hasattr(tokenizer, 'special_tokens_ids'):\n",
    "                        bos_token = tokenizer.special_tokens_ids.get('BOS_None')\n",
    "                        eos_token = tokenizer.special_tokens_ids.get('EOS_None')\n",
    "                        pad_token = tokenizer.special_tokens_ids.get('PAD_None', 0)\n",
    "                    elif hasattr(tokenizer, '__getitem__'):\n",
    "                        # Try dictionary-style access\n",
    "                        try:\n",
    "                            bos_token = tokenizer['BOS_None']\n",
    "                        except (KeyError, TypeError):\n",
    "                            pass\n",
    "                        try:\n",
    "                            eos_token = tokenizer['EOS_None'] \n",
    "                        except (KeyError, TypeError):\n",
    "                            pass\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Build input sequence\n",
    "                if bos_token is not None:\n",
    "                    x = [bos_token] + mel_ids\n",
    "                else:\n",
    "                    x = mel_ids\n",
    "                \n",
    "                # For autoregressive training, y is x shifted by 1\n",
    "                if eos_token is not None:\n",
    "                    y = x[1:] + [eos_token]\n",
    "                else:\n",
    "                    y = x[1:] + [x[-1]]  # Use last token if no EOS\n",
    "                \n",
    "                # Ensure same length\n",
    "                min_len = min(len(x), len(y))\n",
    "                x = x[:min_len]\n",
    "                y = y[:min_len]\n",
    "                \n",
    "                # Pad to max_len if needed\n",
    "                while len(x) < max_len:\n",
    "                    x.append(pad_token)\n",
    "                    y.append(-100)  # Ignore padding in loss\n",
    "                \n",
    "                # Truncate if too long\n",
    "                x = x[:max_len]\n",
    "                y = y[:max_len]\n",
    "                \n",
    "                self.samples.append((x, y))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} samples from {len(files)} files.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        try:\n",
    "            x, y = self.samples[i]\n",
    "            return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "        except:\n",
    "            # Return dummy data if anything fails\n",
    "            return torch.zeros(1024, dtype=torch.long), torch.full((1024,), -100, dtype=torch.long)\n",
    "\n",
    "# ===== Model =====\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb=256, heads=8, layers=6, dropout=0.1, max_len=1024):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_emb = nn.Embedding(vocab_size, emb)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, emb))\n",
    "        \n",
    "        # Use regular transformer layers with causal masking\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb, nhead=heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=layers)\n",
    "        self.lm_head = nn.Linear(emb, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Clamp input tokens to valid range\n",
    "        x = torch.clamp(x, 0, self.vocab_size - 1)\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        tok_emb = self.token_emb(x)\n",
    "        pos_emb = self.pos_emb[:, :seq_len]\n",
    "        embeddings = self.dropout(tok_emb + pos_emb)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
    "        \n",
    "        # Apply transformer\n",
    "        out = self.transformer(embeddings, mask=mask)\n",
    "        return self.lm_head(out)\n",
    "\n",
    "# ===== Train Loop =====\n",
    "def train(model, loader, optimizer, device, clip_grad=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Validate input\n",
    "        if x.max() >= model.vocab_size or x.min() < 0:\n",
    "            print(f\"Warning: Clamping invalid token IDs: min={x.min()}, max={x.max()}\")\n",
    "            x = torch.clamp(x, 0, model.vocab_size - 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(x)\n",
    "        \n",
    "        # Reshape for loss computation\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        y_flat = y.view(-1)\n",
    "        \n",
    "        # Check for NaN/inf\n",
    "        if torch.isnan(logits_flat).any() or torch.isinf(logits_flat).any():\n",
    "            print(f\"NaN/inf detected in logits at batch {batch_idx}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=-100)(logits_flat, y_flat)\n",
    "        \n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"NaN/inf loss at batch {batch_idx}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if clip_grad > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# ===== Generation Functions =====\n",
    "def generate_autoregressive(model, start_tokens, tokenizer, max_len=512, temperature=1.0, top_k=50):\n",
    "    model.eval()\n",
    "    generated = start_tokens[:]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - len(generated)):\n",
    "            # Prepare input\n",
    "            x = torch.tensor(generated, dtype=torch.long).unsqueeze(0)\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = model(x)\n",
    "            logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k filtering if specified\n",
    "            if top_k > 0:\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "                logits = torch.full_like(logits, float('-inf'))\n",
    "                logits[top_k_indices] = top_k_logits\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            \n",
    "            generated.append(next_token)\n",
    "            \n",
    "            # Check for EOS token\n",
    "            eos_token = None\n",
    "            try:\n",
    "                if hasattr(tokenizer, 'special_tokens_ids'):\n",
    "                    if isinstance(tokenizer.special_tokens_ids, dict):\n",
    "                        eos_token = tokenizer.special_tokens_ids.get('EOS_None', None)\n",
    "                    elif hasattr(tokenizer, '__getitem__'):\n",
    "                        try:\n",
    "                            eos_token = tokenizer['EOS_None']\n",
    "                        except (KeyError, TypeError):\n",
    "                            pass\n",
    "                elif hasattr(tokenizer, '__getitem__'):\n",
    "                    try:\n",
    "                        eos_token = tokenizer['EOS_None']\n",
    "                    except (KeyError, TypeError):\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if eos_token is not None and next_token == eos_token:\n",
    "                break\n",
    "    \n",
    "    return generated\n",
    "\n",
    "def save_tokens_to_midi(token_ids, tokenizer, filename=\"generated.mid\"):\n",
    "    try:\n",
    "        # Create TokSequence\n",
    "        seq = TokSequence(ids=token_ids)\n",
    "        \n",
    "        # Decode to score\n",
    "        score = tokenizer.decode([seq])\n",
    "        \n",
    "        # Try different save methods\n",
    "        if hasattr(score, 'dump_midi'):\n",
    "            score.dump_midi(filename)\n",
    "        elif hasattr(score, 'to_midi'):\n",
    "            midi = score.to_midi()\n",
    "            if hasattr(midi, 'write'):\n",
    "                midi.write(filename)\n",
    "            else:\n",
    "                midi.dump(filename)\n",
    "        else:\n",
    "            # Fallback: use miditok's direct conversion\n",
    "            midi = tokenizer.tokens_to_midi([seq])\n",
    "            midi.dump(filename)\n",
    "        \n",
    "        print(f\"Successfully saved MIDI to {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving MIDI: {e}\")\n",
    "        # Try alternative approach\n",
    "        try:\n",
    "            from miditok.utils import tokens_to_midi\n",
    "            midi = tokens_to_midi(token_ids, tokenizer)\n",
    "            midi.dump(filename)\n",
    "            print(f\"Saved MIDI using fallback method to {filename}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback method also failed: {e2}\")\n",
    "\n",
    "# ===== Main Training Script =====\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = REMI()\n",
    "    \n",
    "    # Assuming midi_files is defined elsewhere\n",
    "    # files = midi_files  # Replace with your file list\n",
    "    \n",
    "    # Create dataset and loader\n",
    "    # dataset = MusicDataset(files, tokenizer, max_len=512)\n",
    "    # loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    vocab_size = len(tokenizer.vocab) if hasattr(tokenizer, 'vocab') else tokenizer.vocab_size\n",
    "    model = TransformerDecoder(vocab_size=vocab_size, max_len=512)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        print(\"Using GPU\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    \n",
    "    # Training loop\n",
    "    # for epoch in range(5):\n",
    "    #     print(f\"Epoch {epoch + 1}/5\")\n",
    "    #     train(model, loader, optimizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(\"Model setup complete!\")\n",
    "\n",
    "# ===== Usage Example for Generation =====\n",
    "def generate_music_example(model, tokenizer, loader):\n",
    "    \"\"\"Example of how to generate music after training\"\"\"\n",
    "    \n",
    "    # Debug tokenizer structure\n",
    "    print(\"Tokenizer debug info:\")\n",
    "    print(f\"Type: {type(tokenizer)}\")\n",
    "    if hasattr(tokenizer, 'special_tokens_ids'):\n",
    "        print(f\"special_tokens_ids type: {type(tokenizer.special_tokens_ids)}\")\n",
    "        print(f\"special_tokens_ids: {tokenizer.special_tokens_ids}\")\n",
    "    if hasattr(tokenizer, 'vocab_size'):\n",
    "        print(f\"vocab_size: {tokenizer.vocab_size}\")\n",
    "    if hasattr(tokenizer, 'vocab'):\n",
    "        print(f\"vocab length: {len(tokenizer.vocab)}\")\n",
    "    \n",
    "    # Get a sample from the dataset to use as prompt\n",
    "    sample_batch = next(iter(loader))\n",
    "    start_tokens = sample_batch[0][0][:10].tolist()  # First 10 tokens as prompt\n",
    "    \n",
    "    print(f\"Starting generation with {len(start_tokens)} prompt tokens: {start_tokens}\")\n",
    "    \n",
    "    # Generate\n",
    "    generated_tokens = generate_autoregressive(\n",
    "        model, start_tokens, tokenizer, \n",
    "        max_len=256, temperature=0.8, top_k=40\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(generated_tokens)} tokens\")\n",
    "    \n",
    "    # Save to MIDI\n",
    "    save_tokens_to_midi(generated_tokens, tokenizer, \"generated_music.mid\")\n",
    "    \n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dac1884-0f98-4697-85a8-a4da07164bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer output type: <class 'list'>\n",
      "Length: 1\n",
      "First element type: <class 'miditok.classes.TokSequence'>\n",
      "First element attributes: ['tokens', 'ids', 'bytes', 'events', 'are_ids_encoded', '_ticks_bars', '_ticks_beats', '_ids_decoded']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert\\AppData\\Local\\Temp\\ipykernel_15768\\1630459018.py:21: UserWarning: You are using a depreciated `miditoolkit.MidiFile` object. MidiTokis now (>v3.0.0) using symusic.Score as MIDI backend. Your file willbe converted on the fly, however please consider using symusic.\n",
      "  mel_tokens = tokenizer(midi)  # This returns token IDs or TokSequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing nes_midis\\1154_ff1airs.mid: Could not decode key with 2 flats and mode 255\n",
      "Error processing nes_midis\\1158_ff1fightgs1.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1168_ff1cave1.mid: Could not decode key with 4 flats and mode 255\n",
      "Error processing nes_midis\\1171_ff1cast2.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1182_ff1gurgu.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1211_ff1ship3.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1245_ff2jbat3.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1258_ff2jfin2.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1516_gotcha_level2.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1528_gradius2.mid: Could not decode key with 1 flats and mode 255\n",
      "Error processing nes_midis\\1530_Grad12_1.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1535_gradius5.mid: Could not decode key with 5 flats and mode 255\n",
      "Error processing nes_midis\\1657_Iceclimber.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1725_jplv1.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1728_jplv2_v1.1.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1777_Kirby_Boss.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1808_vegetablevalley.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\1934_ZeldaMelody.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\367_Bbtitle.mid: Could not decode key with 2 flats and mode 255\n",
      "Error processing nes_midis\\837_DD.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\899_dm_fever.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\908_dm_options.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\951_Castle.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\956_dw2town.mid: data byte must be in range 0..127\n",
      "Error processing nes_midis\\985_Zoma3Heavy.mid: data byte must be in range 0..127\n",
      "Loaded 1975 samples from 2000 files.\n",
      "Batch 0, Loss: 5.7311\n",
      "Batch 10, Loss: 5.0567\n",
      "Batch 20, Loss: 4.4447\n",
      "Batch 30, Loss: 4.1975\n",
      "Batch 40, Loss: 3.9840\n",
      "Batch 50, Loss: 3.6261\n",
      "Batch 60, Loss: 3.7053\n",
      "Batch 70, Loss: 2.7516\n",
      "Batch 80, Loss: 3.4220\n",
      "Batch 90, Loss: 3.2647\n",
      "Batch 100, Loss: 3.2020\n",
      "Batch 110, Loss: 3.4355\n",
      "Batch 120, Loss: 3.0870\n",
      "Batch 130, Loss: 2.6180\n",
      "Batch 140, Loss: 2.6052\n",
      "Batch 150, Loss: 2.4025\n",
      "Batch 160, Loss: 2.2145\n",
      "Batch 170, Loss: 2.7183\n",
      "Batch 180, Loss: 2.3154\n",
      "Batch 190, Loss: 2.5386\n",
      "Batch 200, Loss: 2.1777\n",
      "Batch 210, Loss: 2.6710\n",
      "Batch 220, Loss: 2.3566\n",
      "Batch 230, Loss: 2.5973\n",
      "Batch 240, Loss: 2.2368\n",
      "Batch 250, Loss: 2.1686\n",
      "Batch 260, Loss: 2.1502\n",
      "Batch 270, Loss: 2.5255\n",
      "Batch 280, Loss: 1.8975\n",
      "Batch 290, Loss: 2.5066\n",
      "Batch 300, Loss: 2.7053\n",
      "Batch 310, Loss: 2.3395\n",
      "Batch 320, Loss: 2.3504\n",
      "Batch 330, Loss: 2.2268\n",
      "Batch 340, Loss: 2.2965\n",
      "Batch 350, Loss: 2.4884\n",
      "Batch 360, Loss: 2.0870\n",
      "Batch 370, Loss: 2.0316\n",
      "Batch 380, Loss: 2.0344\n",
      "Batch 390, Loss: 1.9311\n",
      "Batch 400, Loss: 2.4222\n",
      "Batch 410, Loss: 2.2135\n",
      "Batch 420, Loss: 2.0683\n",
      "Batch 430, Loss: 2.2275\n",
      "Batch 440, Loss: 2.2798\n",
      "Batch 450, Loss: 1.8560\n",
      "Batch 460, Loss: 1.9437\n",
      "Batch 470, Loss: 1.8725\n",
      "Batch 480, Loss: 2.1587\n",
      "Batch 490, Loss: 2.0612\n",
      "Average loss: 2.6293\n",
      "Batch 0, Loss: 2.2630\n",
      "Batch 10, Loss: 2.0730\n",
      "Batch 20, Loss: 1.7085\n",
      "Batch 30, Loss: 2.1155\n",
      "Batch 40, Loss: 1.8778\n",
      "Batch 50, Loss: 1.7696\n",
      "Batch 60, Loss: 1.8924\n",
      "Batch 70, Loss: 1.6828\n",
      "Batch 80, Loss: 1.8107\n",
      "Batch 90, Loss: 2.1987\n",
      "Batch 100, Loss: 1.5268\n",
      "Batch 110, Loss: 1.8723\n",
      "Batch 120, Loss: 1.7633\n",
      "Batch 130, Loss: 1.6805\n",
      "Batch 140, Loss: 1.7763\n",
      "Batch 150, Loss: 1.7034\n",
      "Batch 160, Loss: 1.5526\n",
      "Batch 170, Loss: 1.8883\n",
      "Batch 180, Loss: 2.0898\n",
      "Batch 190, Loss: 1.5783\n",
      "Batch 200, Loss: 1.6622\n",
      "Batch 210, Loss: 2.5560\n",
      "Batch 220, Loss: 2.1887\n",
      "Batch 230, Loss: 2.0274\n",
      "Batch 240, Loss: 1.8049\n",
      "Batch 250, Loss: 1.6998\n",
      "Batch 260, Loss: 1.8845\n",
      "Batch 270, Loss: 1.9481\n",
      "Batch 280, Loss: 2.1623\n",
      "Batch 290, Loss: 1.8440\n",
      "Batch 300, Loss: 1.7616\n",
      "Batch 310, Loss: 1.7526\n",
      "Batch 320, Loss: 1.7817\n",
      "Batch 330, Loss: 1.8775\n",
      "Batch 340, Loss: 1.9556\n",
      "Batch 350, Loss: 1.8933\n",
      "Batch 360, Loss: 1.8112\n",
      "Batch 370, Loss: 2.2229\n",
      "Batch 380, Loss: 1.7394\n",
      "Batch 390, Loss: 1.5929\n",
      "Batch 400, Loss: 1.9480\n",
      "Batch 410, Loss: 1.6032\n",
      "Batch 420, Loss: 1.9342\n",
      "Batch 430, Loss: 1.7866\n",
      "Batch 440, Loss: 2.0894\n",
      "Batch 450, Loss: 1.8346\n",
      "Batch 460, Loss: 1.9414\n",
      "Batch 470, Loss: 1.6679\n",
      "Batch 480, Loss: 1.5941\n",
      "Batch 490, Loss: 1.7768\n",
      "Average loss: 1.8797\n",
      "Batch 0, Loss: 1.7076\n",
      "Batch 10, Loss: 1.4892\n",
      "Batch 20, Loss: 1.5977\n",
      "Batch 30, Loss: 1.4196\n",
      "Batch 40, Loss: 1.5068\n",
      "Batch 50, Loss: 1.6864\n",
      "Batch 60, Loss: 1.9354\n",
      "Batch 70, Loss: 1.8775\n",
      "Batch 80, Loss: 1.6661\n",
      "Batch 90, Loss: 1.7565\n",
      "Batch 100, Loss: 1.7809\n",
      "Batch 110, Loss: 1.7256\n",
      "Batch 120, Loss: 1.4404\n",
      "Batch 130, Loss: 1.7828\n",
      "Batch 140, Loss: 1.6477\n",
      "Batch 150, Loss: 1.7060\n",
      "Batch 160, Loss: 1.6354\n",
      "Batch 170, Loss: 1.2914\n",
      "Batch 180, Loss: 1.9478\n",
      "Batch 190, Loss: 1.5056\n",
      "Batch 200, Loss: 1.6890\n",
      "Batch 210, Loss: 1.8108\n",
      "Batch 220, Loss: 1.7235\n",
      "Batch 230, Loss: 1.6203\n",
      "Batch 240, Loss: 1.5972\n",
      "Batch 250, Loss: 1.8620\n",
      "Batch 260, Loss: 1.2909\n",
      "Batch 270, Loss: 1.5233\n",
      "Batch 280, Loss: 1.7348\n",
      "Batch 290, Loss: 1.4529\n",
      "Batch 300, Loss: 1.4207\n",
      "Batch 310, Loss: 2.0896\n",
      "Batch 320, Loss: 1.8509\n",
      "Batch 330, Loss: 1.9334\n",
      "Batch 340, Loss: 1.4984\n",
      "Batch 350, Loss: 1.6564\n",
      "Batch 360, Loss: 1.7692\n",
      "Batch 370, Loss: 2.2800\n",
      "Batch 380, Loss: 1.7824\n",
      "Batch 390, Loss: 1.6954\n",
      "Batch 400, Loss: 1.6633\n",
      "Batch 410, Loss: 1.6652\n",
      "Batch 420, Loss: 1.4111\n",
      "Batch 430, Loss: 1.7769\n",
      "Batch 440, Loss: 1.6129\n",
      "Batch 450, Loss: 1.6217\n",
      "Batch 460, Loss: 1.3567\n",
      "Batch 470, Loss: 1.4530\n",
      "Batch 480, Loss: 1.7412\n",
      "Batch 490, Loss: 1.8177\n",
      "Average loss: 1.6851\n",
      "Batch 0, Loss: 1.6456\n",
      "Batch 10, Loss: 1.5760\n",
      "Batch 20, Loss: 1.7876\n",
      "Batch 30, Loss: 1.6718\n",
      "Batch 40, Loss: 1.4133\n",
      "Batch 50, Loss: 1.4049\n",
      "Batch 60, Loss: 1.7394\n",
      "Batch 70, Loss: 1.7486\n",
      "Batch 80, Loss: 1.5120\n",
      "Batch 90, Loss: 1.5189\n",
      "Batch 100, Loss: 1.4078\n",
      "Batch 110, Loss: 1.3121\n",
      "Batch 120, Loss: 1.6420\n",
      "Batch 130, Loss: 1.4815\n",
      "Batch 140, Loss: 1.2138\n",
      "Batch 150, Loss: 1.5567\n",
      "Batch 160, Loss: 1.3426\n",
      "Batch 170, Loss: 1.8022\n",
      "Batch 180, Loss: 1.5551\n",
      "Batch 190, Loss: 1.6382\n",
      "Batch 200, Loss: 1.0567\n",
      "Batch 210, Loss: 1.4691\n",
      "Batch 220, Loss: 1.5058\n",
      "Batch 230, Loss: 1.4567\n",
      "Batch 240, Loss: 1.1770\n",
      "Batch 250, Loss: 1.5005\n",
      "Batch 260, Loss: 2.0722\n",
      "Batch 270, Loss: 1.1536\n",
      "Batch 280, Loss: 1.5140\n",
      "Batch 290, Loss: 1.6827\n",
      "Batch 300, Loss: 1.4237\n",
      "Batch 310, Loss: 1.2144\n",
      "Batch 320, Loss: 1.0347\n",
      "Batch 330, Loss: 1.4068\n",
      "Batch 340, Loss: 1.3774\n",
      "Batch 350, Loss: 1.8419\n",
      "Batch 360, Loss: 1.3917\n",
      "Batch 370, Loss: 1.3547\n",
      "Batch 380, Loss: 1.7516\n",
      "Batch 390, Loss: 1.6245\n",
      "Batch 400, Loss: 1.4171\n",
      "Batch 410, Loss: 1.4064\n",
      "Batch 420, Loss: 1.4937\n",
      "Batch 430, Loss: 1.4602\n",
      "Batch 440, Loss: 1.5215\n",
      "Batch 450, Loss: 1.2623\n",
      "Batch 460, Loss: 1.3638\n",
      "Batch 470, Loss: 1.5756\n",
      "Batch 480, Loss: 1.2748\n",
      "Batch 490, Loss: 1.5673\n",
      "Average loss: 1.4776\n",
      "Batch 0, Loss: 1.2635\n",
      "Batch 10, Loss: 1.5507\n",
      "Batch 20, Loss: 1.4298\n",
      "Batch 30, Loss: 1.1509\n",
      "Batch 40, Loss: 1.5473\n",
      "Batch 50, Loss: 1.3270\n",
      "Batch 60, Loss: 1.0426\n",
      "Batch 70, Loss: 0.9828\n",
      "Batch 80, Loss: 1.1099\n",
      "Batch 90, Loss: 1.3143\n",
      "Batch 100, Loss: 1.0574\n",
      "Batch 110, Loss: 1.1110\n",
      "Batch 120, Loss: 1.5926\n",
      "Batch 130, Loss: 1.6319\n",
      "Batch 140, Loss: 1.0459\n",
      "Batch 150, Loss: 1.2005\n",
      "Batch 160, Loss: 1.3365\n",
      "Batch 170, Loss: 1.2101\n",
      "Batch 180, Loss: 1.1165\n",
      "Batch 190, Loss: 1.1519\n",
      "Batch 200, Loss: 1.5821\n",
      "Batch 210, Loss: 1.4131\n",
      "Batch 220, Loss: 0.8230\n",
      "Batch 230, Loss: 1.2548\n",
      "Batch 240, Loss: 1.3580\n",
      "Batch 250, Loss: 1.3642\n",
      "Batch 260, Loss: 1.1432\n",
      "Batch 270, Loss: 1.2019\n",
      "Batch 280, Loss: 1.3583\n",
      "Batch 290, Loss: 1.2914\n",
      "Batch 300, Loss: 1.4523\n",
      "Batch 310, Loss: 1.1725\n",
      "Batch 320, Loss: 1.1596\n",
      "Batch 330, Loss: 1.1357\n",
      "Batch 340, Loss: 1.5236\n",
      "Batch 350, Loss: 1.1430\n",
      "Batch 360, Loss: 1.2409\n",
      "Batch 370, Loss: 1.1663\n",
      "Batch 380, Loss: 1.0118\n",
      "Batch 390, Loss: 1.3250\n",
      "Batch 400, Loss: 1.5035\n",
      "Batch 410, Loss: 2.7080\n",
      "Batch 420, Loss: 1.1842\n",
      "Batch 430, Loss: 1.3844\n",
      "Batch 440, Loss: 1.0380\n",
      "Batch 450, Loss: 1.7486\n",
      "Batch 460, Loss: 1.2249\n",
      "Batch 470, Loss: 1.2446\n",
      "Batch 480, Loss: 1.1963\n",
      "Batch 490, Loss: 0.9547\n",
      "Average loss: 1.3009\n"
     ]
    }
   ],
   "source": [
    "# 1. Train the model\n",
    "tokenizer = REMI()\n",
    "files = midi_files\n",
    "dataset = MusicDataset(files, tokenizer, max_len=512)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "model = TransformerDecoder(vocab_size=tokenizer.vocab_size, max_len=512).cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train for a few epochs\n",
    "for epoch in range(5):\n",
    "    train(model, loader, optimizer, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5812522a-9598-4d31-a8ab-549664aa845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer debug info:\n",
      "Type: <class 'miditok.tokenizations.remi.REMI'>\n",
      "special_tokens_ids type: <class 'list'>\n",
      "special_tokens_ids: [0, 1, 2, 3]\n",
      "vocab_size: 282\n",
      "vocab length: 282\n",
      "Starting generation with 10 prompt tokens: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Generated 256 tokens\n",
      "Successfully saved MIDI to generated_music.mid\n"
     ]
    }
   ],
   "source": [
    "# 2. Generate music\n",
    "generated_tokens = generate_music_example(model, tokenizer, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a41e07e-0759-444f-bf59-df28485e6bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticks per beat: 8\n",
      "Tempo changes: 1\n",
      "Number of tracks: 1\n",
      "Track 0: 24 notes, program 0\n",
      "  Pitch range: 40-62\n",
      "  Average volume: 103.0/127\n",
      "  First note: pitch 62, start 3872\n",
      "Total notes: 24\n",
      "MIDI contains notes and should make sound\n"
     ]
    }
   ],
   "source": [
    "midi = miditoolkit.MidiFile(\"generated_music.mid\")\n",
    "print(f\"Ticks per beat: {midi.ticks_per_beat}\")\n",
    "print(f\"Tempo changes: {len(midi.tempo_changes)}\")\n",
    "print(f\"Number of tracks: {len(midi.instruments)}\")\n",
    "\n",
    "total_notes = 0\n",
    "for i, track in enumerate(midi.instruments):\n",
    "    num_notes = len(track.notes)\n",
    "    total_notes += num_notes\n",
    "    print(f\"Track {i}: {num_notes} notes, program {track.program}\")\n",
    "    \n",
    "    if num_notes > 0:\n",
    "        pitches = [note.pitch for note in track.notes]\n",
    "        velocities = [note.velocity for note in track.notes]\n",
    "        avg_velocity = sum(velocities) / len(velocities)\n",
    "        print(f\"  Pitch range: {min(pitches)}-{max(pitches)}\")\n",
    "        print(f\"  Average volume: {avg_velocity:.1f}/127\")\n",
    "        print(f\"  First note: pitch {track.notes[0].pitch}, start {track.notes[0].start}\")\n",
    "\n",
    "print(f\"Total notes: {total_notes}\")\n",
    "if total_notes == 0:\n",
    "    print(\"No notes found\")\n",
    "else:\n",
    "    print(\"MIDI contains notes and should make sound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23610fbc-68db-4be1-8a48-0611a88bb4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
